{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From PCA to Autoencoders: Unsupervised representation learning\n",
    "\n",
    "ISAE-Supaero fili√®re SDD\n",
    "\n",
    "Florent FOREST\n",
    "\n",
    "forest@lipn.univ-paris13.fr\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/FlorentF9/Supaero-mlautoencoders.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print('Torch version ', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is CUDA is available\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(MNIST('../data', train=True, download=True, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(MNIST('../data', train=False, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's code 1\n",
    "\n",
    "<div style=\"color: #191; font-size: 16px; background-color: #dfd; padding: 20px; border-radius: 15px\">\n",
    "<p><b>Exercise</b></p>\n",
    "<p>Implement a multi-layer (MLP) autoencoder using PyTorch by completing the following code snippet.</p>\n",
    "<ol>\n",
    "    <li>Try changing the number of layers, units per layer, and activation function, and observe the impact on the loss function.</li>\n",
    "    <li>The following code uses a <b>gaussian MLP</b>, i.e. <b>linear output activation</b> + <b>mean squared error (MSE) loss</b>. Could we also use a Bernoulli distribution, with a <b>sigmoid output activation</b> + <b>cross-entropy loss</b>?</li>\n",
    "    <li>Visualize the latent space using t-SNE, and compare it with the original data.</li>\n",
    "</ol>\n",
    "    \n",
    "<p>If you have time left, you can also implement a <b>convolutional autoencoder</b>. You might guess it, convolutional and pooling layers will replace the fully-connected layers in the encoder. And what about the decoder?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 200),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 10)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 200),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 784)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "#loss_function = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (X_batch, _) in enumerate(train_loader):\n",
    "        X_batch = X_batch.to(device).view(-1, 784)\n",
    "        # forward\n",
    "        X_pred = model(X_batch)\n",
    "        loss = loss_function(X_pred, X_batch)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader.dataset))\n",
    "            \n",
    "    print('Epoch: {} - Train loss: {}'.format(epoch, train_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (X_batch, _) in enumerate(test_loader):\n",
    "        X_batch = X_batch.to(device).view(-1, 784)\n",
    "        # forward\n",
    "        X_pred = model(X_batch)\n",
    "        test_loss += loss_function(X_pred, X_batch).item()\n",
    "        \n",
    "        if i == 0:\n",
    "            n = min(X_batch.size(0), 8)\n",
    "            comparison = torch.cat([X_batch.view(batch_size, 1, 28, 28)[:n], X_pred.view(batch_size, 1, 28, 28)[:n]])\n",
    "            img = comparison.cpu().numpy()\n",
    "\n",
    "print('Test loss: {}'.format(test_loss / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize original VS reconstructed samples\n",
    "n = img.shape[0] // 2\n",
    "fig, ax = plt.subplots(2, n, figsize=(12, 3))\n",
    "for i in range(n):\n",
    "    ax[0][i].imshow(img[i, 0], cmap='gray')\n",
    "    ax[1][i].imshow(img[n + i, 0], cmap='gray')\n",
    "    ax[0][i].axis('off')\n",
    "    ax[1][i].axis('off')\n",
    "plt.subplots_adjust(hspace=0.0, wspace=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(iter(test_loader))\n",
    "X_test = np.vstack([x[0].view(-1, 784).numpy() for x in test_list])\n",
    "y_test = np.vstack([x[1].view(-1, 1).numpy() for x in test_list])\n",
    "del test_list\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a smaller sample for running t-SNE\n",
    "N = 1000\n",
    "sample_idx = np.random.permutation(np.arange(X_test.shape[0]))[:N]\n",
    "X_sample = X_test[sample_idx]\n",
    "y_sample = y_test[sample_idx].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "# apply t-SNE on raw samples\n",
    "raw_tsne = tsne.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Z_sample = model.encode(torch.Tensor(X_sample).to(device)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply t-SNE on encoded samples\n",
    "ae_tsne = tsne.fit_transform(Z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "for label in range(10):\n",
    "    ax[0].scatter(raw_tsne[y_sample == label, 0], raw_tsne[y_sample == label, 1], label=label, cmap='Accent')\n",
    "    ax[1].scatter(ae_tsne[y_sample == label, 0], ae_tsne[y_sample == label, 1], label=label, cmap='Accent')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('t-SNE visualization of raw MNIST')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('t-SNE visualization of MNIST in AE latent space');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact on $k$-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "\n",
    "def _contingency_matrix(y_true, y_pred):\n",
    "    w = np.zeros((y_true.max() + 1, y_pred.max() + 1), dtype=np.int64)\n",
    "    for c, k in zip(y_true, y_pred):\n",
    "        w[c, k] += 1  # w[c, k] = number of c-labeled samples in cluster k\n",
    "    return w\n",
    "\n",
    "def clustering_accuracy(y_true, y_pred):\n",
    "    \"\"\"Unsupervised clustering accuracy.\n",
    "\n",
    "    Can only be used if the number of target classes in y_true is equal to the number of clusters in y_pred.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n]\n",
    "        true labels.\n",
    "    y_pred : array, shape = [n]\n",
    "        predicted cluster ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float in [0,1] (higher is better)\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    y_pred = y_pred.astype(np.int64)\n",
    "    w = _contingency_matrix(y_true, y_pred).T\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return np.sum([w[i, j] for i, j in ind]) / y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "y_pred = KMeans(n_clusters=10, n_jobs=-1).fit_predict(X_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z_test = model.encode(torch.Tensor(X_test).to(device)).cpu().numpy()\n",
    "y_pred_ae = KMeans(n_clusters=10, n_jobs=-1).fit_predict(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_accuracy(y_test, y_pred_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Let's code 2\n",
    "\n",
    "<div style=\"color: #191; font-size: 16px; background-color: #dfd; padding: 20px; border-radius: 15px\">\n",
    "<p><b>Exercise</b></p>\n",
    "<p>Implement a variational autoencoder (VAE) using PyTorch by completing the following code snippet.</p>\n",
    "<ol>\n",
    "    <li>Implement the reparameterization.</li>\n",
    "    <li>Implement the VAE loss function. As in the standard AE, the reconstruction error can be either a <b>mean squared error (MSE)</b> (gaussian decoder) or a <b>binary cross-entropy loss</b> (Bernoulli decoder)</li>\n",
    "    <li>Visualize the latent space using t-SNE, and compare it with the original data.</li>\n",
    "</ol>\n",
    "    \n",
    "<p>If you have time left, you can also implement a <b>convolutional VAE</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.mu = nn.Linear(400, 20)\n",
    "        self.logvar = nn.Linear(400, 20)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 400),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(400, 784)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(decoded_x, x, mu, logvar):\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    # reconstruction error (MSE or BCE)\n",
    "    RE = 0.0 # TODO\n",
    "    \n",
    "    # Kullback-Leibler divergence error\n",
    "    # see VAE paper: Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = 0.0 # TODO\n",
    "\n",
    "    return RE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (X_batch, _) in enumerate(train_loader):\n",
    "        X_batch = X_batch.to(device).view(-1, 784)\n",
    "        # forward\n",
    "        X_pred, mu, logvar = model(X_batch)\n",
    "        loss = loss_function(X_pred, X_batch, mu, logvar)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader.dataset))\n",
    "            \n",
    "    print('Epoch: {} - Train loss: {}'.format(epoch, train_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (X_batch, _) in enumerate(test_loader):\n",
    "        X_batch = X_batch.to(device).view(-1, 784)\n",
    "        # forward\n",
    "        X_pred, mu, logvar = model(X_batch)\n",
    "        test_loss += loss_function(X_pred, X_batch, mu, logvar).item()\n",
    "        \n",
    "        if i == 0:\n",
    "            n = min(X_batch.size(0), 8)\n",
    "            comparison = torch.cat([X_batch.view(batch_size, 1, 28, 28)[:n], X_pred.view(batch_size, 1, 28, 28)[:n]])\n",
    "            img = comparison.cpu().numpy()\n",
    "\n",
    "print('Test loss: {}'.format(test_loss / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize original VS reconstructed samples\n",
    "n = img.shape[0] // 2\n",
    "fig, ax = plt.subplots(2, n, figsize=(12, 3))\n",
    "for i in range(n):\n",
    "    ax[0][i].imshow(img[i, 0], cmap='gray')\n",
    "    ax[1][i].imshow(img[n + i, 0], cmap='gray')\n",
    "    ax[0][i].axis('off')\n",
    "    ax[1][i].axis('off')\n",
    "plt.subplots_adjust(hspace=0.0, wspace=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some samples\n",
    "n_samples = 8\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(n_samples, 20).to(device)\n",
    "    sample = model.decode(z).view(-1, 28, 28).cpu().numpy()\n",
    "\n",
    "_, ax = plt.subplots(1, n_samples, figsize=(12, 4))\n",
    "for i in range(n_samples):\n",
    "    ax[i].imshow(sample[i], cmap='gray')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Z_sample = model.encode(torch.Tensor(X_sample).to(device))[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply t-SNE on encoded samples\n",
    "vae_tsne = tsne.fit_transform(Z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "for label in range(10):\n",
    "    ax[0].scatter(raw_tsne[y_sample == label, 0], raw_tsne[y_sample == label, 1], label=label, cmap='Accent')\n",
    "    ax[1].scatter(vae_tsne[y_sample == label, 0], vae_tsne[y_sample == label, 1], label=label, cmap='Accent')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('t-SNE visualization of raw MNIST')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('t-SNE visualization of MNIST in VAE latent space');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact on $k$-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=10, n_jobs=-1).fit_predict(X_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z_test = model.encode(torch.Tensor(X_test).to(device))[0].cpu().numpy()  \n",
    "y_pred_vae = KMeans(n_clusters=10, n_jobs=-1).fit_predict(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_accuracy(y_test, y_pred_vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
