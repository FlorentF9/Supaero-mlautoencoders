@article{Oja1982,
abstract = {A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence.},
author = {Oja, Erkki},
doi = {10.1007/BF00275687},
issn = {14321416},
journal = {Journal of Mathematical Biology},
keywords = {Neuron models,Stochastic approximation,Synaptic plasticity},
number = {3},
pages = {267--273},
title = {{A Simplified Neuron Model as a Principal Component Analyzer}},
volume = {15},
year = {1982}
}
@article{Becker1991,
author = {Becker, Suzanna},
issn = {0007-1250},
journal = {The International Journal of Neural Systems},
pages = {17--33},
title = {{Unsupervised Learning Procedures for Neural Networks}},
volume = {1},
year = {1991}
}
@misc{Oja1992,
author = {Oja, Erkki},
booktitle = {Neural Networks},
pages = {927--935},
title = {{Principal Components, Minor Components, and Linear Neural Networks}},
url = {https://users.ics.aalto.fi/oja/Oja92.pdf},
volume = {5},
year = {1992}
}
@article{Hinton2006,
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan},
doi = {10.1126/science.1127647},
eprint = {20},
isbn = {3135786504},
issn = {0036-8075},
journal = {Science},
keywords = {autoencoder,deep,dimensionality reduction},
number = {July},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
volume = {313},
year = {2006}
}
@article{Erhan2010,
author = {Erhan, Dumitru and Courville, Aaron and Vincent, Pascal},
journal = {Journal of Machine Learning Research},
pages = {625--660},
title = {{Why Does Unsupervised Pre-training Help Deep Learning ?}},
volume = {11},
year = {2010}
}
@article{Vincent2010,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpass-ing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordi-nary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {autoencoder,autoencoders,deep,deep belief networks,deep learning,denoising,dimensionality reduction,unsupervised feature learning},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@techreport{Ng2011,
author = {Ng, Andrew},
booktitle = {CS294A Lecture notes},
institution = {Stanford University},
keywords = {autoencoder},
mendeley-tags = {autoencoder},
title = {{Sparse autoencoder}},
url = {https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf},
year = {2011}
}
@article{Makhzani2013,
abstract = {Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.},
archivePrefix = {arXiv},
arxivId = {1312.5663},
author = {Makhzani, Alireza and Frey, Brendan},
eprint = {1312.5663},
title = {{k-Sparse Autoencoders}},
url = {http://arxiv.org/abs/1312.5663},
year = {2013}
}
@article{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
isbn = {2004012439},
issn = {0004-6361},
journal = {2nd International Conference on Learning Representations (ICLR)},
pages = {1--14},
title = {{Stochastic Gradient VB and the Variational Auto-Encoder}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@article{Arpit2016,
abstract = {Sparse distributed representation is the key to learning useful features in deep learning algorithms, because not only it is an efficient mode of data representation, but also - more importantly - it captures the generation process of most real world data. While a number of regularized autoencoders (AE) enforce sparsity explicitly in their learned representation and others don't, there has been little formal analysis on what encourages sparsity in these models in general. Our objective is to formally study this general problem for regularized auto-encoders. We provide sufficient conditions on both regularization and activation functions that encourage sparsity. We show that multiple popular models (de-noising and contractive auto encoders, e.g.) and activations (rectified linear and sigmoid, e.g.) satisfy these conditions; thus, our conditions help explain sparsity in their learned representation. Thus our theoretical and empirical analysis together shed light on the properties of regularization/activation that are conductive to sparsity and unify a number of existing auto-encoder models and activation functions under the same analytical framework.},
archivePrefix = {arXiv},
arxivId = {1505.05561},
author = {Arpit, Devansh and Zhou, Yingbo and Ngo, Hung Q. and Govindaraju, Venu},
eprint = {1505.05561},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {211--223},
title = {{Why regularized auto-encoders learn sparse representation?}},
volume = {1},
year = {2016}
}
@inproceedings{Malhotra2016,
abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
archivePrefix = {arXiv},
arxivId = {1607.00148},
author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
booktitle = {ICML 2016 Anomaly Detection Workshop},
eprint = {1607.00148},
isbn = {1285483},
keywords = {Anomaly Detection,LSTM Encoder Decoder},
title = {{LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection}},
url = {http://arxiv.org/abs/1607.00148},
year = {2016}
}
@inproceedings{Devries2017,
author = {Devries, Terrance and Taylor, Graham W},
booktitle = {ICLR 2017 Workshop},
keywords = {autoencoder,deep},
pages = {1--12},
title = {{Dataset Augmentation in Feature Space}},
year = {2017}
}
@article{Plaut2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.10253v3},
author = {Plaut, Elad},
eprint = {arXiv:1804.10253v3},
pages = {1--6},
title = {{From Principal Subspaces to Principal Components with Linear Autoencoders}},
year = {2018}
}
