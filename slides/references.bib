@article{Oja1982,
abstract = {A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence.},
author = {Oja, Erkki},
doi = {10.1007/BF00275687},
issn = {14321416},
journal = {Journal of Mathematical Biology},
keywords = {Neuron models,Stochastic approximation,Synaptic plasticity},
number = {3},
pages = {267--273},
title = {{A Simplified Neuron Model as a Principal Component Analyzer}},
volume = {15},
year = {1982}
}
@article{Becker1991,
author = {Becker, Suzanna},
issn = {0007-1250},
journal = {The International Journal of Neural Systems},
pages = {17--33},
title = {{Unsupervised Learning Procedures for Neural Networks}},
volume = {1},
year = {1991}
}
@misc{Oja1992,
author = {Oja, Erkki},
booktitle = {Neural Networks},
pages = {927--935},
title = {{Principal Components, Minor Components, and Linear Neural Networks}},
url = {https://users.ics.aalto.fi/oja/Oja92.pdf},
volume = {5},
year = {1992}
}
@article{Hinton2006,
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan},
doi = {10.1126/science.1127647},
eprint = {20},
isbn = {3135786504},
issn = {0036-8075},
journal = {Science},
keywords = {autoencoder,deep,dimensionality reduction},
number = {July},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
volume = {313},
year = {2006}
}
@article{Vincent2010,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpass-ing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordi-nary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {autoencoder,autoencoders,deep,deep belief networks,deep learning,denoising,dimensionality reduction,unsupervised feature learning},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@article{Erhan2010,
author = {Erhan, Dumitru and Courville, Aaron and Vincent, Pascal},
journal = {Journal of Machine Learning Research},
pages = {625--660},
title = {{Why Does Unsupervised Pre-training Help Deep Learning ?}},
volume = {11},
year = {2010}
}
@article{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
isbn = {2004012439},
issn = {0004-6361},
journal = {2nd International Conference on Learning Representations (ICLR)},
pages = {1--14},
title = {{Stochastic Gradient VB and the Variational Auto-Encoder}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@inproceedings{Oord2017,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {NIPS},
eprint = {1711.00937},
issn = {10495258},
title = {{Neural Discrete Representation Learning}},
url = {http://arxiv.org/abs/1711.00937},
year = {2017}
}
@inproceedings{Devries2017,
author = {Devries, Terrance and Taylor, Graham W},
booktitle = {ICLR 2017 Workshop},
keywords = {autoencoder,deep},
pages = {1--12},
title = {{Dataset Augmentation in Feature Space}},
year = {2017}
}
@article{Plaut2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.10253v3},
author = {Plaut, Elad},
eprint = {arXiv:1804.10253v3},
pages = {1--6},
title = {{From Principal Subspaces to Principal Components with Linear Autoencoders}},
year = {2018}
}
